{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bkgsur/sciml/blob/main/GNN_Physics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGKCXbWIVjT6"
      },
      "source": [
        "# Simulating Complex Physics with Graph Networks: step by step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1E_PEIxaHr8"
      },
      "source": [
        "## Overview\n",
        "\n",
        "By Peng Chen, Shiyu Li, and Haochen Shi as part of the Stanford CS224W course project. This tutorial provides a step-by-step guide for how to build a Graph Network to simulate complex physics.\n",
        "\n",
        "**Before we get started:**\n",
        "- This Colab includes a concise PyG implementation of the paper ***Learning to Simulate Complex Physics with Graph Networks*. We adapted our code from the open-source tensorflow implementation by DeepMind.\n",
        "    - Link to the pdf of this paper: https://arxiv.org/abs/2002.09405\n",
        "    - Link to Deepmind's implementation: https://github.com/deepmind/deepmind-research/tree/master/learning_to_simulate\n",
        "    - Link to the video site by DeepMind: https://sites.google.com/view/learning-to-simulate\n",
        "    Meduim Article - https://medium.com/stanford-cs224w/simulating-complex-physics-with-graph-networks-step-by-step-177354cb9b05\n",
        "- Make sure to **sequentially run all the cells in each section**, so that the intermediate variables / packages will carry over to the next cell.\n",
        "- Feel free to make a copy to your own drive to play around with it! Have fun with this tutorial!\n",
        "- Dataset URL -  https://drive.google.com/drive/folders/11uuYl0peqPg2DQno64YPYMODPu8fjDXU?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGMo0gTp-GLe"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "print(f\"PyTorch has version {torch.__version__} with cuda {torch.version.cuda}\")\n",
        "TORCH_CUDA= torch.__version__\n",
        "!pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.5.1+cu121.html\n",
        "!pip install torch_geometric"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Z4HbK-OPUK8"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "The dataset WaterDropSmall includes 100 videos of dropping water to the ground rendered in a particle-based physics simulator. It is a cropped version of the WaterDrop dataset by Deepmind. We will download this dataset from Google Cloud stoarge to the folder `temp/datasets` in the file system. You can inspect the downloaded files on the **Files** menu on the left of this Colab.\n",
        "\n",
        "The `metadata.json` file in the dataset includes the following information:\n",
        "1. The sequence length of each video data point\n",
        "2. The dimensionality, 2d or 3d\n",
        "3. The box bounds, which specify the bounding box for the scene\n",
        "4. The default connectivity radius, which defines the size of each particle's neighborhood\n",
        "5. The statistics for normalization, such as the mean and standard deviation of the velocity and acceleration of particles\n",
        "\n",
        "\n",
        "Each data point in the dataset includes the following information:\n",
        "1. The type of the particles, such as water\n",
        "2. The particle positions at each frame in the video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxevKO1yPb4d"
      },
      "source": [
        "DATASET_NAME = \"WaterDropSmall\"\n",
        "OUTPUT_DIR = os.path.join(\"temp\", \"datasets\", DATASET_NAME)\n",
        "if not os.path.exists(OUTPUT_DIR):\n",
        "  !mkdir -p \"$OUTPUT_DIR\"\n",
        "else:\n",
        "  print(f\"{OUTPUT_DIR} exists!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NW0_YsEPG68T"
      },
      "source": [
        "## Data Preprocessing\n",
        "\n",
        "Since we cannot apply the raw data in the dataset to train the GNN model directly, we need to go through the following steps to convert the raw data into graphs with descriptive node features and edge features:\n",
        "1. Apply noise to the trajectory to have more diverse training examples\n",
        "1. Construct the graph based on the distance between particles\n",
        "1. Extract node-level features: particle velocities and their distance to the boundary\n",
        "1. Extract edge-level features: displacement and distance between particles\n",
        "\n",
        "If you are not interested in the data pipeline, your can skip to the end of this section. There is a detailed explanation and visualization of one data point."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCy3zaaOGrrS"
      },
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import torch_geometric as pyg\n",
        "\n",
        "def generate_noise(position_seq, noise_std):\n",
        "    \"\"\"Generate noise for a trajectory\"\"\"\n",
        "    velocity_seq = position_seq[:, 1:] - position_seq[:, :-1]\n",
        "    time_steps = velocity_seq.size(1)\n",
        "    velocity_noise = torch.randn_like(velocity_seq) * (noise_std / time_steps ** 0.5)\n",
        "    velocity_noise = velocity_noise.cumsum(dim=1)\n",
        "    position_noise = velocity_noise.cumsum(dim=1)\n",
        "    position_noise = torch.cat((torch.zeros_like(position_noise)[:, 0:1], position_noise), dim=1)\n",
        "    return position_noise\n",
        "\n",
        "\n",
        "def preprocess(particle_type, position_seq, target_position, metadata, noise_std):\n",
        "    \"\"\"Preprocess a trajectory and construct the graph\"\"\"\n",
        "    # apply noise to the trajectory\n",
        "    position_noise = generate_noise(position_seq, noise_std)\n",
        "    position_seq = position_seq + position_noise\n",
        "\n",
        "    # calculate the velocities of particles\n",
        "    recent_position = position_seq[:, -1]\n",
        "    velocity_seq = position_seq[:, 1:] - position_seq[:, :-1]\n",
        "\n",
        "    # construct the graph based on the distances between particles\n",
        "    n_particle = recent_position.size(0)\n",
        "    edge_index = pyg.nn.radius_graph(recent_position, metadata[\"default_connectivity_radius\"], loop=True, max_num_neighbors=n_particle)\n",
        "\n",
        "    # node-level features: velocity, distance to the boundary\n",
        "    normal_velocity_seq = (velocity_seq - torch.tensor(metadata[\"vel_mean\"])) / torch.sqrt(torch.tensor(metadata[\"vel_std\"]) ** 2 + noise_std ** 2)\n",
        "    boundary = torch.tensor(metadata[\"bounds\"])\n",
        "    distance_to_lower_boundary = recent_position - boundary[:, 0]\n",
        "    distance_to_upper_boundary = boundary[:, 1] - recent_position\n",
        "    distance_to_boundary = torch.cat((distance_to_lower_boundary, distance_to_upper_boundary), dim=-1)\n",
        "    distance_to_boundary = torch.clip(distance_to_boundary / metadata[\"default_connectivity_radius\"], -1.0, 1.0)\n",
        "\n",
        "    # edge-level features: displacement, distance\n",
        "    dim = recent_position.size(-1)\n",
        "    edge_displacement = (torch.gather(recent_position, dim=0, index=edge_index[0].unsqueeze(-1).expand(-1, dim)) -\n",
        "                   torch.gather(recent_position, dim=0, index=edge_index[1].unsqueeze(-1).expand(-1, dim)))\n",
        "    edge_displacement /= metadata[\"default_connectivity_radius\"]\n",
        "    edge_distance = torch.norm(edge_displacement, dim=-1, keepdim=True)\n",
        "\n",
        "    # ground truth for training\n",
        "    if target_position is not None:\n",
        "        last_velocity = velocity_seq[:, -1]\n",
        "        next_velocity = target_position + position_noise[:, -1] - recent_position\n",
        "        acceleration = next_velocity - last_velocity\n",
        "        acceleration = (acceleration - torch.tensor(metadata[\"acc_mean\"])) / torch.sqrt(torch.tensor(metadata[\"acc_std\"]) ** 2 + noise_std ** 2)\n",
        "    else:\n",
        "        acceleration = None\n",
        "\n",
        "    # return the graph with features\n",
        "    graph = pyg.data.Data(\n",
        "        x=particle_type,\n",
        "        edge_index=edge_index,\n",
        "        edge_attr=torch.cat((edge_displacement, edge_distance), dim=-1),\n",
        "        y=acceleration,\n",
        "        pos=torch.cat((velocity_seq.reshape(velocity_seq.size(0), -1), distance_to_boundary), dim=-1)\n",
        "    )\n",
        "    return graph"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqfx4NcguDEY"
      },
      "source": [
        "### One Step Dataset\n",
        "\n",
        "Each datapoint in this dataset contains trajectories sliced to short time windows. We will use this dataset in the training phase because the history of particles' states are necessary for the model to make predictions. But in the meantime, since long-horizon prediction is usually inaccurate and time-consuming, we sliced the trajectories to short time windows to improve the perfomance of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2HrUjPnsF_4"
      },
      "source": [
        "class OneStepDataset(pyg.data.Dataset):\n",
        "    def __init__(self, data_path, split, window_length=7, noise_std=0.0, return_pos=False):\n",
        "        super().__init__()\n",
        "        try:\n",
        "            # load dataset from the disk\n",
        "            with open(os.path.join(data_path, \"metadata.json\")) as f:\n",
        "                self.metadata = json.load(f)\n",
        "            with open(os.path.join(data_path, f\"{split}_offset.json\")) as f:\n",
        "                self.offset = json.load(f)\n",
        "            self.offset = {int(k): v for k, v in self.offset.items()}\n",
        "            self.window_length = window_length\n",
        "            self.noise_std = noise_std\n",
        "            self.return_pos = return_pos\n",
        "\n",
        "            self.particle_type = np.memmap(os.path.join(data_path, f\"{split}_particle_type.dat\"), dtype=np.int64, mode=\"r\")\n",
        "            self.position = np.memmap(os.path.join(data_path, f\"{split}_position.dat\"), dtype=np.float32, mode=\"r\")\n",
        "\n",
        "            for traj in self.offset.values():\n",
        "                self.dim = traj[\"position\"][\"shape\"][2]\n",
        "                break\n",
        "\n",
        "            # cut particle trajectories according to time slices\n",
        "            self.windows = []\n",
        "            for traj in self.offset.values():\n",
        "                size = traj[\"position\"][\"shape\"][1]\n",
        "                length = traj[\"position\"][\"shape\"][0] - window_length + 1\n",
        "                for i in range(length):\n",
        "                    desc = {\n",
        "                        \"size\": size,\n",
        "                        \"type\": traj[\"particle_type\"][\"offset\"],\n",
        "                        \"pos\": traj[\"position\"][\"offset\"] + i * size * self.dim,\n",
        "                    }\n",
        "                    self.windows.append(desc)\n",
        "        except:\n",
        "           raise\n",
        "    def len(self):\n",
        "        return len(self.windows)\n",
        "\n",
        "    def get(self, idx):\n",
        "        # load corresponding data for this time slice\n",
        "        window = self.windows[idx]\n",
        "        size = window[\"size\"]\n",
        "        particle_type = self.particle_type[window[\"type\"]: window[\"type\"] + size].copy()\n",
        "        particle_type = torch.from_numpy(particle_type)\n",
        "        position_seq = self.position[window[\"pos\"]: window[\"pos\"] + self.window_length * size * self.dim].copy()\n",
        "        position_seq.resize(self.window_length, size, self.dim)\n",
        "        position_seq = position_seq.transpose(1, 0, 2)\n",
        "        target_position = position_seq[:, -1]\n",
        "        position_seq = position_seq[:, :-1]\n",
        "        target_position = torch.from_numpy(target_position)\n",
        "        position_seq = torch.from_numpy(position_seq)\n",
        "\n",
        "        # construct the graph\n",
        "        with torch.no_grad():\n",
        "            graph = preprocess(particle_type, position_seq, target_position, self.metadata, self.noise_std)\n",
        "        if self.return_pos:\n",
        "          return graph, position_seq[:, -1]\n",
        "        return graph"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVjrldn4kD-P"
      },
      "source": [
        "### Visualize a graph in the dataset\n",
        "\n",
        "Each data point in the dataset is a `pyg.data.Data` object which describes a graph. We explain the contents of the first data point, and visualize the graph."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4MJuOhjkTjx"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "\n",
        "dataset_sample = OneStepDataset(OUTPUT_DIR, \"valid\", return_pos=True)\n",
        "graph, position = dataset_sample[0]\n",
        "\n",
        "print(f\"The first item in the valid set is a graph: {graph}\")\n",
        "print(f\"This graph has {graph.num_nodes} nodes and {graph.num_edges} edges.\")\n",
        "print(f\"Each node is a particle and each edge is the interaction between two particles.\")\n",
        "print(f\"Each node has {graph.num_node_features} categorial feature (Data.x), which represents the type of the node.\")\n",
        "print(f\"Each node has a {graph.pos.size(1)}-dim feature vector (Data.pos), which represents the positions and velocities of the particle (node) in several frames.\")\n",
        "print(f\"Each edge has a {graph.num_edge_features}-dim feature vector (Data.edge_attr), which represents the relative distance and displacement between particles.\")\n",
        "print(f\"The model is expected to predict a {graph.y.size(1)}-dim vector for each node (Data.y), which represents the acceleration of the particle.\")\n",
        "\n",
        "# remove directions of edges, because it is a symmetric directed graph.\n",
        "nx_graph = pyg.utils.to_networkx(graph).to_undirected()\n",
        "# remove self loops, because every node has a self loop.\n",
        "nx_graph.remove_edges_from(nx.selfloop_edges(nx_graph))\n",
        "plt.figure(figsize=(7, 7))\n",
        "nx.draw(nx_graph, pos={i: tuple(v) for i, v in enumerate(position)}, node_size=50)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqhPcZeKthYq"
      },
      "source": [
        "### Rollout Dataset\n",
        "\n",
        "Each datapoint in this dataset contains trajectories of particles over 1000 time frames. This dataset is used in the evaluation phase to measure the model's ability to makie long-horizon predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kuk2Z-I8sFv7"
      },
      "source": [
        "class RolloutDataset(pyg.data.Dataset):\n",
        "    def __init__(self, data_path, split, window_length=7):\n",
        "        super().__init__()\n",
        "\n",
        "        # load data from the disk\n",
        "        with open(os.path.join(data_path, \"metadata.json\")) as f:\n",
        "            self.metadata = json.load(f)\n",
        "        with open(os.path.join(data_path, f\"{split}_offset.json\")) as f:\n",
        "            self.offset = json.load(f)\n",
        "        self.offset = {int(k): v for k, v in self.offset.items()}\n",
        "        self.window_length = window_length\n",
        "\n",
        "        self.particle_type = np.memmap(os.path.join(data_path, f\"{split}_particle_type.dat\"), dtype=np.int64, mode=\"r\")\n",
        "        self.position = np.memmap(os.path.join(data_path, f\"{split}_position.dat\"), dtype=np.float32, mode=\"r\")\n",
        "\n",
        "        for traj in self.offset.values():\n",
        "            self.dim = traj[\"position\"][\"shape\"][2]\n",
        "            break\n",
        "\n",
        "    def len(self):\n",
        "        return len(self.offset)\n",
        "\n",
        "    def get(self, idx):\n",
        "        traj = self.offset[idx]\n",
        "        size = traj[\"position\"][\"shape\"][1]\n",
        "        time_step = traj[\"position\"][\"shape\"][0]\n",
        "        particle_type = self.particle_type[traj[\"particle_type\"][\"offset\"]: traj[\"particle_type\"][\"offset\"] + size].copy()\n",
        "        particle_type = torch.from_numpy(particle_type)\n",
        "        position = self.position[traj[\"position\"][\"offset\"]: traj[\"position\"][\"offset\"] + time_step * size * self.dim].copy()\n",
        "        position.resize(traj[\"position\"][\"shape\"])\n",
        "        position = torch.from_numpy(position)\n",
        "        data = {\"particle_type\": particle_type, \"position\": position}\n",
        "        return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nlnk5URCF7pZ"
      },
      "source": [
        "## GNN Model\n",
        "\n",
        "We will walk through the implementation of the GNN model in this section!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vsVI6epJcsn"
      },
      "source": [
        "### Helper class\n",
        "\n",
        "We first define a class for Multi-Layer Perceptron (MLP). This class generates an MLP given the width and the depth of it. Because MLPs are used in several places of the GNN, this helper class will make the code cleaner."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ht-upXnRo0dV"
      },
      "source": [
        "import math\n",
        "import torch_scatter\n",
        "\n",
        "class MLP(torch.nn.Module):\n",
        "    \"\"\"Multi-Layer perceptron\"\"\"\n",
        "    def __init__(self, input_size, hidden_size, output_size, layers, layernorm=True):\n",
        "        super().__init__()\n",
        "        self.layers = torch.nn.ModuleList()\n",
        "        for i in range(layers):\n",
        "            self.layers.append(torch.nn.Linear(\n",
        "                input_size if i == 0 else hidden_size,\n",
        "                output_size if i == layers - 1 else hidden_size,\n",
        "            ))\n",
        "            if i != layers - 1:\n",
        "                self.layers.append(torch.nn.ReLU())\n",
        "        if layernorm:\n",
        "            self.layers.append(torch.nn.LayerNorm(output_size))\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for layer in self.layers:\n",
        "            if isinstance(layer, torch.nn.Linear):\n",
        "                layer.weight.data.normal_(0, 1 / math.sqrt(layer.in_features))\n",
        "                layer.bias.data.fill_(0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_pkzDgqJ_ED"
      },
      "source": [
        "### GNN layers\n",
        "\n",
        "In the following code block, we implement one type of GNN layer named `InteractionNetwork` (IN), which is proposed by the paper *Interaction Networks for Learning about Objects,\n",
        "Relations and Physics*.\n",
        "\n",
        "For a graph $G$, let the feature of node $i$ be $v_i$, and the feature of edge $(i, j)$ be $e_{i, j}$. There are three stages for IN to generate new features of nodes and edges.\n",
        "\n",
        "1. **Message generation.** If there is an edge pointing from node $i$ to node $j$, node $i$ sends a message to node $j$. The message carries the information of the edge and its two nodes, so it is generated by the following equation $\\mathrm{Msg}_{i,j} = \\mathrm{MLP}(v_i, v_j, e_{i,j})$.\n",
        "\n",
        "1. **Message aggregation.** In this stage, each node of the graph aggregates all the messages that it received to a fixed-sized representation. In the IN, aggregation means summing all the messages up, i.e., $\\mathrm{Agg}_i=\\sum_{(j,i)\\in G}\\mathrm{Msg}_{i,j}$.\n",
        "\n",
        "1. **Update.** Finally, we update features of nodes and edges with the results of previous stages. For each edge, its new feature is simply the sum of its old feature and the correspond message, i.e., $e'_{i,j}=e_{i,j}+\\mathrm{Msg}_{i,j}$. For each node, the new feature is determined by its old feature and the aggregated message, i.e., $v'_i=v_i+\\mathrm{MLP}(v_i, \\mathrm{Agg}_i)$.\n",
        "\n",
        "In PyG, GNN layers are implemented as subclass of `MessagePassing`. We need to override three critical functions to implement our `InteractionNetwork` GNN layer. Each function corresponds to one stage of the GNN layer.\n",
        "\n",
        "1. `message()` -> message generation\n",
        "\n",
        "  This function controls how a message is generated on each edge of the graph. It takes three arguments: (1) `x_i`, features of the source nodes; (2) `x_j`, features of the target nodes; and (3) `edge_feature`, features of the edges themselves. In the IN, we simply concatenate all these features and generate the messages with an MLP.\n",
        "\n",
        "1. `aggregate()` -> message aggregation\n",
        "\n",
        "  This function aggregates messages for nodes. It depends on two arguments: (1) `inputs`, messages; and (2) `index`, the graph structure. We handle over the task of message aggregation to the function `torch_scatter.scatter` and specifies in the argument `reduce` that we want to sum messages up. Because we want to retain messages themselves to update edge features, we return both messages and aggregated messages.\n",
        "\n",
        "1. `forward()` -> update\n",
        "\n",
        "  This function puts everything together. `x` is the node features, `edge_index` is the graph structure and `edge_feature` is edge features. The function`MessagePassing.propagate` invokes functions `message` and `aggregate` for us. Then, we update node features and edge features and return them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nobE0LcXJ6RR"
      },
      "source": [
        "class InteractionNetwork(pyg.nn.MessagePassing):\n",
        "    \"\"\"Interaction Network as proposed in this paper:\n",
        "    https://proceedings.neurips.cc/paper/2016/hash/3147da8ab4a0437c15ef51a5cc7f2dc4-Abstract.html\"\"\"\n",
        "    def __init__(self, hidden_size, layers):\n",
        "        super().__init__()\n",
        "        self.lin_edge = MLP(hidden_size * 3, hidden_size, hidden_size, layers)\n",
        "        self.lin_node = MLP(hidden_size * 2, hidden_size, hidden_size, layers)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_feature):\n",
        "        edge_out, aggr = self.propagate(edge_index, x=(x, x), edge_feature=edge_feature)\n",
        "        node_out = self.lin_node(torch.cat((x, aggr), dim=-1))\n",
        "        edge_out = edge_feature + edge_out\n",
        "        node_out = x + node_out\n",
        "        return node_out, edge_out\n",
        "\n",
        "    def message(self, x_i, x_j, edge_feature):\n",
        "        x = torch.cat((x_i, x_j, edge_feature), dim=-1)\n",
        "        x = self.lin_edge(x)\n",
        "        return x\n",
        "\n",
        "    def aggregate(self, inputs, index, dim_size=None):\n",
        "        out = torch_scatter.scatter(inputs, index, dim=self.node_dim, dim_size=dim_size, reduce=\"sum\")\n",
        "        return (inputs, out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-Aa9znXKH40"
      },
      "source": [
        "### The GNN\n",
        "\n",
        "Now its time to stack GNN layers to a GNN. Besides GNN layers, there are pre-processing and post-processing blocks in the GNN. Before GNN layers, input features are transformed by MLP so that the expressiveness of GNN is improved without increasing GNN layers. After GNN layers, final outputs (accelerations of particles in our case) are extracted from features generated by GNN layers to meet the requirement of the task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoB4_A6YJ7FP"
      },
      "source": [
        "class LearnedSimulator(torch.nn.Module):\n",
        "    \"\"\"Graph Network-based Simulators(GNS)\"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        hidden_size=128,\n",
        "        n_mp_layers=10, # number of GNN layers\n",
        "        num_particle_types=9,\n",
        "        particle_type_dim=16, # embedding dimension of particle types\n",
        "        dim=2, # dimension of the world, typical 2D or 3D\n",
        "        window_size=5, # the model looks into W frames before the frame to be predicted\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.window_size = window_size\n",
        "        self.embed_type = torch.nn.Embedding(num_particle_types, particle_type_dim)\n",
        "        self.node_in = MLP(particle_type_dim + dim * (window_size + 2), hidden_size, hidden_size, 3)\n",
        "        self.edge_in = MLP(dim + 1, hidden_size, hidden_size, 3)\n",
        "        self.node_out = MLP(hidden_size, hidden_size, dim, 3, layernorm=False)\n",
        "        self.n_mp_layers = n_mp_layers\n",
        "        self.layers = torch.nn.ModuleList([InteractionNetwork(\n",
        "            hidden_size, 3\n",
        "        ) for _ in range(n_mp_layers)])\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        torch.nn.init.xavier_uniform_(self.embed_type.weight)\n",
        "\n",
        "    def forward(self, data):\n",
        "        # pre-processing\n",
        "        # node feature: combine categorial feature data.x and contiguous feature data.pos.\n",
        "        node_feature = torch.cat((self.embed_type(data.x), data.pos), dim=-1)\n",
        "        node_feature = self.node_in(node_feature)\n",
        "        edge_feature = self.edge_in(data.edge_attr)\n",
        "        # stack of GNN layers\n",
        "        for i in range(self.n_mp_layers):\n",
        "            node_feature, edge_feature = self.layers[i](node_feature, data.edge_index, edge_feature=edge_feature)\n",
        "        # post-processing\n",
        "        out = self.node_out(node_feature)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7LUIouahvhW"
      },
      "source": [
        "## Training\n",
        "\n",
        "Before we start training the model, let's configure the hyperparameters! Since the accessible computaion power is limited in Colab, we will only run 1 epoch of training, which takes about 1.5 hour. Consequently, we won't be able to produce as accurate results as shown in the original paper in this Colab. Alternatively, we provide a checkpoint of training the model on the entire WaterDrop dataset for 5 epochs, which takes about 14 hours with a GeForce RTX 3080 Ti."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Czpr3hJTiCuC"
      },
      "source": [
        "data_path = OUTPUT_DIR\n",
        "model_path = os.path.join(\"temp\", \"models\", DATASET_NAME)\n",
        "rollout_path = os.path.join(\"temp\", \"rollouts\", DATASET_NAME)\n",
        "\n",
        "!mkdir -p \"$model_path\"\n",
        "!mkdir -p \"$rollout_path\"\n",
        "\n",
        "params = {\n",
        "    \"epoch\": 1,\n",
        "    \"batch_size\": 4,\n",
        "    \"lr\": 1e-4,\n",
        "    \"noise\": 3e-4,\n",
        "    \"save_interval\": 1000,\n",
        "    \"eval_interval\": 1000,\n",
        "    \"rollout_interval\": 200000,\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gN_P6D4tK7FQ"
      },
      "source": [
        "Below are some helper functions for evaluation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSIAlxN7KvuZ"
      },
      "source": [
        "def rollout(model, data, metadata, noise_std):\n",
        "    device = next(model.parameters()).device\n",
        "    model.eval()\n",
        "    window_size = model.window_size + 1\n",
        "    total_time = data[\"position\"].size(0)\n",
        "    traj = data[\"position\"][:window_size]\n",
        "    traj = traj.permute(1, 0, 2)\n",
        "    particle_type = data[\"particle_type\"]\n",
        "\n",
        "    for time in range(total_time - window_size):\n",
        "        with torch.no_grad():\n",
        "            graph = preprocess(particle_type, traj[:, -window_size:], None, metadata, 0.0)\n",
        "            graph = graph.to(device)\n",
        "            acceleration = model(graph).cpu()\n",
        "            acceleration = acceleration * torch.sqrt(torch.tensor(metadata[\"acc_std\"]) ** 2 + noise_std ** 2) + torch.tensor(metadata[\"acc_mean\"])\n",
        "\n",
        "            recent_position = traj[:, -1]\n",
        "            recent_velocity = recent_position - traj[:, -2]\n",
        "            new_velocity = recent_velocity + acceleration\n",
        "            new_position = recent_position + new_velocity\n",
        "            traj = torch.cat((traj, new_position.unsqueeze(1)), dim=1)\n",
        "\n",
        "    return traj\n",
        "\n",
        "\n",
        "def oneStepMSE(simulator, dataloader, metadata, noise):\n",
        "    \"\"\"Returns two values, loss and MSE\"\"\"\n",
        "    total_loss = 0.0\n",
        "    total_mse = 0.0\n",
        "    batch_count = 0\n",
        "    simulator.eval()\n",
        "    with torch.no_grad():\n",
        "        scale = torch.sqrt(torch.tensor(metadata[\"acc_std\"]) ** 2 + noise ** 2).cuda()\n",
        "        for data in valid_loader:\n",
        "            data = data.cuda()\n",
        "            pred = simulator(data)\n",
        "            mse = ((pred - data.y) * scale) ** 2\n",
        "            mse = mse.sum(dim=-1).mean()\n",
        "            loss = ((pred - data.y) ** 2).mean()\n",
        "            total_mse += mse.item()\n",
        "            total_loss += loss.item()\n",
        "            batch_count += 1\n",
        "    return total_loss / batch_count, total_mse / batch_count\n",
        "\n",
        "\n",
        "def rolloutMSE(simulator, dataset, noise):\n",
        "    total_loss = 0.0\n",
        "    batch_count = 0\n",
        "    simulator.eval()\n",
        "    with torch.no_grad():\n",
        "        for rollout_data in dataset:\n",
        "            rollout_out = rollout(simulator, rollout_data, dataset.metadata, noise)\n",
        "            rollout_out = rollout_out.permute(1, 0, 2)\n",
        "            loss = (rollout_out - rollout_data[\"position\"]) ** 2\n",
        "            loss = loss.sum(dim=-1).mean()\n",
        "            total_loss += loss.item()\n",
        "            batch_count += 1\n",
        "    return total_loss / batch_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwvseROOFqt0"
      },
      "source": [
        "Here is the main training loop!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRsKEIX6XAwN"
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def train(params, simulator, train_loader, valid_loader, valid_rollout_dataset):\n",
        "    loss_fn = torch.nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(simulator.parameters(), lr=params[\"lr\"])\n",
        "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.1 ** (1 / 5e6))\n",
        "\n",
        "    # recording loss curve\n",
        "    train_loss_list = []\n",
        "    eval_loss_list = []\n",
        "    onestep_mse_list = []\n",
        "    rollout_mse_list = []\n",
        "    total_step = 0\n",
        "\n",
        "    for i in range(params[\"epoch\"]):\n",
        "        simulator.train()\n",
        "        progress_bar = tqdm(train_loader, desc=f\"Epoch {i}\")\n",
        "        total_loss = 0\n",
        "        batch_count = 0\n",
        "        for data in progress_bar:\n",
        "            optimizer.zero_grad()\n",
        "            data = data.cuda()\n",
        "            pred = simulator(data)\n",
        "            loss = loss_fn(pred, data.y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            total_loss += loss.item()\n",
        "            batch_count += 1\n",
        "            progress_bar.set_postfix({\"loss\": loss.item(), \"avg_loss\": total_loss / batch_count, \"lr\": optimizer.param_groups[0][\"lr\"]})\n",
        "            total_step += 1\n",
        "            train_loss_list.append((total_step, loss.item()))\n",
        "\n",
        "            # evaluation\n",
        "            if total_step % params[\"eval_interval\"] == 0:\n",
        "                simulator.eval()\n",
        "                eval_loss, onestep_mse = oneStepMSE(simulator, valid_loader, valid_dataset.metadata, params[\"noise\"])\n",
        "                eval_loss_list.append((total_step, eval_loss))\n",
        "                onestep_mse_list.append((total_step, onestep_mse))\n",
        "                tqdm.write(f\"\\nEval: Loss: {eval_loss}, One Step MSE: {onestep_mse}\")\n",
        "                simulator.train()\n",
        "\n",
        "            # do rollout on valid set\n",
        "            if total_step % params[\"rollout_interval\"] == 0:\n",
        "                simulator.eval()\n",
        "                rollout_mse = rolloutMSE(simulator, valid_rollout_dataset, params[\"noise\"])\n",
        "                rollout_mse_list.append((total_step, rollout_mse))\n",
        "                tqdm.write(f\"\\nEval: Rollout MSE: {rollout_mse}\")\n",
        "                simulator.train()\n",
        "\n",
        "            # save model\n",
        "            if total_step % params[\"save_interval\"] == 0:\n",
        "                torch.save(\n",
        "                    {\n",
        "                        \"model\": simulator.state_dict(),\n",
        "                        \"optimizer\": optimizer.state_dict(),\n",
        "                        \"scheduler\": scheduler.state_dict(),\n",
        "                    },\n",
        "                    os.path.join(model_path, f\"checkpoint_{total_step}.pt\")\n",
        "                )\n",
        "    return train_loss_list, eval_loss_list, onestep_mse_list, rollout_mse_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZ-U-nlEakHF"
      },
      "source": [
        "Finally, let's load the dataset and train the model! It takes roughly 1.5 hour to run this block on Colab with the default parameters. **If you are impatient, we highly recommend you to skip the next 2 blocks and load the checkpoint we provided to save some time; otherwise, make a cup of tea/coffee and come back later to see the results of training!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1HWNWqbE6db"
      },
      "source": [
        "# Training the model is time-consuming. We highly recommend you to skip this block and load the checkpoint in the next block.\n",
        "\n",
        "# load dataset\n",
        "train_dataset = OneStepDataset(data_path, \"train\", noise_std=params[\"noise\"])\n",
        "valid_dataset = OneStepDataset(data_path, \"valid\", noise_std=params[\"noise\"])\n",
        "train_loader = pyg.loader.DataLoader(train_dataset, batch_size=params[\"batch_size\"], shuffle=True, pin_memory=True, num_workers=2)\n",
        "valid_loader = pyg.loader.DataLoader(valid_dataset, batch_size=params[\"batch_size\"], shuffle=False, pin_memory=True, num_workers=2)\n",
        "valid_rollout_dataset = RolloutDataset(data_path, \"valid\")\n",
        "\n",
        "# build model\n",
        "simulator = LearnedSimulator()\n",
        "simulator = simulator.cuda()\n",
        "\n",
        "# train the model\n",
        "train_loss_list, eval_loss_list, onestep_mse_list, rollout_mse_list = train(params, simulator, train_loader, valid_loader, valid_rollout_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBw_qGjz62_S"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# visualize the loss curve\n",
        "plt.figure()\n",
        "plt.plot(*zip(*train_loss_list), label=\"train\")\n",
        "plt.plot(*zip(*eval_loss_list), label=\"valid\")\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Y2Pf_TcbR0K"
      },
      "source": [
        "Load the checkpoint trained by us. Do **not** run this block if you have trained your model in the previous block."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cw26wx-9B_Q3"
      },
      "source": [
        "simulator = LearnedSimulator()\n",
        "simulator = simulator.cuda()\n",
        "\n",
        "!wget -O temp/models/WaterDrop_checkpoint.pt https://storage.googleapis.com/cs224w_course_project_dataset/Checkpoints/WaterDrop_checkpoint.pt\n",
        "checkpoint = torch.load(\"temp/models/WaterDrop_checkpoint.pt\")\n",
        "simulator.load_state_dict(checkpoint[\"model\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eDD-SySxKFt"
      },
      "source": [
        "## Visualization\n",
        "\n",
        "Since the video is 1000 frames long, it might take a few minutes to rollout."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNhrML1SrWIe"
      },
      "source": [
        "rollout_dataset = RolloutDataset(data_path, \"valid\")\n",
        "simulator.eval()\n",
        "rollout_data = rollout_dataset[0]\n",
        "rollout_out = rollout(simulator, rollout_data, rollout_dataset.metadata, params[\"noise\"])\n",
        "rollout_out = rollout_out.permute(1, 0, 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0W5Tm4rws0_"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import animation\n",
        "from IPython.display import HTML\n",
        "\n",
        "TYPE_TO_COLOR = {\n",
        "    3: \"black\",\n",
        "    0: \"green\",\n",
        "    7: \"magenta\",\n",
        "    6: \"gold\",\n",
        "    5: \"blue\",\n",
        "}\n",
        "\n",
        "\n",
        "def visualize_prepare(ax, particle_type, position, metadata):\n",
        "    bounds = metadata[\"bounds\"]\n",
        "    ax.set_xlim(bounds[0][0], bounds[0][1])\n",
        "    ax.set_ylim(bounds[1][0], bounds[1][1])\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    ax.set_aspect(1.0)\n",
        "    points = {type_: ax.plot([], [], \"o\", ms=2, color=color)[0] for type_, color in TYPE_TO_COLOR.items()}\n",
        "    return ax, position, points\n",
        "\n",
        "\n",
        "def visualize_pair(particle_type, position_pred, position_gt, metadata):\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "    plot_info = [\n",
        "        visualize_prepare(axes[0], particle_type, position_gt, metadata),\n",
        "        visualize_prepare(axes[1], particle_type, position_pred, metadata),\n",
        "    ]\n",
        "    axes[0].set_title(\"Ground truth\")\n",
        "    axes[1].set_title(\"Prediction\")\n",
        "\n",
        "    plt.close()\n",
        "\n",
        "    def update(step_i):\n",
        "        outputs = []\n",
        "        for _, position, points in plot_info:\n",
        "            for type_, line in points.items():\n",
        "                mask = particle_type == type_\n",
        "                line.set_data(position[step_i, mask, 0], position[step_i, mask, 1])\n",
        "            outputs.append(line)\n",
        "        return outputs\n",
        "\n",
        "    return animation.FuncAnimation(fig, update, frames=np.arange(0, position_gt.size(0)), interval=10, blit=True)\n",
        "\n",
        "anim = visualize_pair(rollout_data[\"particle_type\"], rollout_out, rollout_data[\"position\"], rollout_dataset.metadata)\n",
        "HTML(anim.to_html5_video())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3t753E3BzEC"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "Hope this Colab is helpful for you to understand how to apply GNN in a real-world application such as simulating complex physics! If you're interested in the technical details, please read our [medium post](https://) or look at the [original paper](https://arxiv.org/abs/2002.09405) by DeepMind. Thanks for spending your time with us!"
      ]
    }
  ]
}